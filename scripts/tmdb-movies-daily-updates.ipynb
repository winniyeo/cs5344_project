{"cells":[{"cell_type":"code","execution_count":7,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-05T15:28:58.65958Z","iopub.status.busy":"2024-04-05T15:28:58.65918Z","iopub.status.idle":"2024-04-05T15:28:58.665105Z","shell.execute_reply":"2024-04-05T15:28:58.663964Z","shell.execute_reply.started":"2024-04-05T15:28:58.659547Z"},"trusted":true},"outputs":[],"source":["import requests"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import json\n","import concurrent.futures\n","import threading\n","import time\n","import logging\n","import os\n","from tqdm import tqdm\n","import datetime\n","import pandas as pd\n","import csv\n","import subprocess"]},{"cell_type":"markdown","metadata":{},"source":["## Kaggle Secrets\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T15:28:58.668179Z","iopub.status.busy":"2024-04-05T15:28:58.667474Z","iopub.status.idle":"2024-04-05T15:28:59.337713Z","shell.execute_reply":"2024-04-05T15:28:59.336754Z","shell.execute_reply.started":"2024-04-05T15:28:58.668138Z"},"trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'kaggle_secrets'","output_type":"error","traceback":["\u001b[31m---------------------------------------------------------------------------\u001b[39m","\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkaggle_secrets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UserSecretsClient\n\u001b[32m      2\u001b[39m user_secrets = UserSecretsClient()\n\u001b[32m      3\u001b[39m kaggle_apikey = user_secrets.get_secret(\u001b[33m\"\u001b[39m\u001b[33mkaggle_apikey\u001b[39m\u001b[33m\"\u001b[39m)\n","\u001b[31mModuleNotFoundError\u001b[39m: No module named 'kaggle_secrets'"]}],"source":["from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","kaggle_apikey = user_secrets.get_secret(\"kaggle_apikey\")\n","kaggle_username = user_secrets.get_secret(\"kaggle_username\")\n","tmdb_key = user_secrets.get_secret(\"tmdb_key\")\n","\n","os.environ['KAGGLE_USERNAME'] = kaggle_username\n","os.environ['KAGGLE_KEY'] = kaggle_apikey"]},{"cell_type":"markdown","metadata":{},"source":["## Basic file configurations for the project\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T15:28:59.339208Z","iopub.status.busy":"2024-04-05T15:28:59.338923Z","iopub.status.idle":"2024-04-05T15:28:59.347495Z","shell.execute_reply":"2024-04-05T15:28:59.346324Z","shell.execute_reply.started":"2024-04-05T15:28:59.339184Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/5b/p6kdbqhn57dbt096gwlqtkjh0000gn/T/ipykernel_70412/1483241695.py:19: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n","  date_today = datetime.datetime.utcnow().date()\n"]}],"source":["# Get current working directory\n","cwd = os.getcwd()\n","\n","# Define folder paths\n","data_folder = os.path.join(cwd, 'TMDB_movie_data')\n","archive_folder = os.path.join(data_folder, 'TMDB_archive')\n","output_folder = os.path.join(data_folder, 'temp_movies')\n","log_file_path = os.path.join(data_folder, 'logs')\n","saved_movie_tracker_path = os.path.join(\n","    archive_folder, 'completed_movie_ids.txt')\n","\n","# Create necessary folders if they don't exist\n","os.makedirs(data_folder, exist_ok=True)\n","os.makedirs(output_folder, exist_ok=True)\n","os.makedirs(log_file_path, exist_ok=True)\n","os.makedirs(archive_folder, exist_ok=True)\n","\n","# Get current date\n","date_today = datetime.datetime.utcnow().date()\n","date_today_str = str(date_today)\n","\n","# Create file path for saving json file\n","json_save_file_path = os.path.join(\n","    archive_folder, f'{date_today_str}_TMDB_movies.ndjson')"]},{"cell_type":"markdown","metadata":{},"source":["## Kaggle File Paths\n"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T15:28:59.349104Z","iopub.status.busy":"2024-04-05T15:28:59.34881Z","iopub.status.idle":"2024-04-05T15:28:59.360112Z","shell.execute_reply":"2024-04-05T15:28:59.359141Z","shell.execute_reply.started":"2024-04-05T15:28:59.349079Z"},"trusted":true},"outputs":[],"source":["# folder path for Kaggle files\n","kaggle_folder = os.path.join(data_folder, 'to_kaggle')\n","os.makedirs(kaggle_folder, exist_ok=True)\n","\n","# folder path for Kaggle dataset zip files\n","kaggle_zip_folder = os.path.join(kaggle_folder, 'TMDB_movie_dataset_v11')\n","os.makedirs(kaggle_zip_folder, exist_ok=True)\n","\n","# file path for Kaggle dataset CSV file\n","kaggle_file = os.path.join(\n","    kaggle_zip_folder, 'TMDB_movie_dataset_v11.csv')"]},{"cell_type":"markdown","metadata":{},"source":["## Global variables\n"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T15:28:59.362928Z","iopub.status.busy":"2024-04-05T15:28:59.362606Z","iopub.status.idle":"2024-04-05T15:29:10.497955Z","shell.execute_reply":"2024-04-05T15:29:10.496905Z","shell.execute_reply.started":"2024-04-05T15:28:59.362902Z"},"trusted":true},"outputs":[],"source":["# variable to store the number of movies collected\n","movies_collected = 0\n","lock = threading.Lock()\n","dataset_df = pd.read_csv('/kaggle/input/tmdb-movies-dataset-2023-930k-movies/TMDB_movie_dataset_v11.csv')\n","full_log = False # Mark True to log everything"]},{"cell_type":"markdown","metadata":{},"source":["## Setup logger\n"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T15:29:10.499617Z","iopub.status.busy":"2024-04-05T15:29:10.499198Z","iopub.status.idle":"2024-04-05T15:29:10.505659Z","shell.execute_reply":"2024-04-05T15:29:10.504673Z","shell.execute_reply.started":"2024-04-05T15:29:10.499581Z"},"trusted":true},"outputs":[],"source":["def setup_logger() -> logging:\n","    # Create a logger for the current module\n","    logger = logging.getLogger(__name__)\n","    logger.setLevel(logging.INFO)\n","\n","    # Define the log format\n","    log_formatter = logging.Formatter(\n","        '%(asctime)s    | %(name)s  | %(levelname)s | %(message)s')\n","\n","    # Create a stream handler to output log messages to the console\n","    stream_handler = logging.StreamHandler()\n","    stream_handler.setFormatter(log_formatter)\n","    logger.addHandler(stream_handler)\n","\n","    return logger"]},{"cell_type":"markdown","metadata":{},"source":["## Get the latest movie id\n"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T15:29:10.507376Z","iopub.status.busy":"2024-04-05T15:29:10.507044Z","iopub.status.idle":"2024-04-05T15:29:10.515118Z","shell.execute_reply":"2024-04-05T15:29:10.514417Z","shell.execute_reply.started":"2024-04-05T15:29:10.507348Z"},"trusted":true},"outputs":[],"source":["def get_latest() -> int:\n","\n","    url = \"https://api.themoviedb.org/3/movie/latest\"\n","    headers = {\n","        \"accept\": \"application/json\",\n","        \"Authorization\": f\"Bearer {tmdb_key}\"\n","    }\n","\n","    # Retry the request until it succeeds\n","    retries = 0\n","    while retries < 500:\n","        try:\n","            response = requests.get(url, headers=headers)\n","            break\n","        except:\n","            retries += 1\n","            time.sleep(5)\n","\n","    # Extract the ID of the latest movie from the response\n","    response_in_json = json.loads(response.text)\n","    movies_id = int(response_in_json['id'])\n","\n","    return movies_id"]},{"cell_type":"markdown","metadata":{},"source":["## Get the last movie id from my dataset\n"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T15:29:10.517088Z","iopub.status.busy":"2024-04-05T15:29:10.516464Z","iopub.status.idle":"2024-04-05T15:29:10.527258Z","shell.execute_reply":"2024-04-05T15:29:10.526049Z","shell.execute_reply.started":"2024-04-05T15:29:10.517051Z"},"trusted":true},"outputs":[],"source":["def get_oldest() -> int:\n","    global dataset_df\n","    \n","    # Get the 'id' value of the last row in the sorted DataFrame\n","    dataset_df.sort_values(['id'],inplace=True)\n","    last_id = dataset_df.iloc[-1]['id']\n","    \n","    return last_id"]},{"cell_type":"markdown","metadata":{},"source":["## Delete old temp files.\n","\n","Not needed on kaggle, but when running locally, this is a must\n"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T15:29:10.529503Z","iopub.status.busy":"2024-04-05T15:29:10.529123Z","iopub.status.idle":"2024-04-05T15:29:10.537354Z","shell.execute_reply":"2024-04-05T15:29:10.536683Z","shell.execute_reply.started":"2024-04-05T15:29:10.529474Z"},"trusted":true},"outputs":[],"source":["def delete_old_files(logger: logging) -> None:\n","    logger.info(\"Deleting Old temp files\")\n","    try:\n","        # Get a list of files from the last run in the output folder\n","        files_from_last_run = os.listdir(output_folder)\n","        \n","        # delete old temp files\n","        if len(files_from_last_run) > 0:\n","            for file in files_from_last_run:\n","                full_file_path = os.path.join(output_folder, file)\n","                os.remove(full_file_path)\n","    except Exception as error:\n","        logger.error(f\"Error when deleting old temp files: {error}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Main function that handles getting movie data\n"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T15:29:10.539009Z","iopub.status.busy":"2024-04-05T15:29:10.538281Z","iopub.status.idle":"2024-04-05T15:29:10.553304Z","shell.execute_reply":"2024-04-05T15:29:10.552418Z","shell.execute_reply.started":"2024-04-05T15:29:10.538981Z"},"trusted":true},"outputs":[],"source":["def get_keywords(movie_id: int) -> dict | None:\n","    \"\"\"\n","    Gets keywords from TMDB\n","    \"\"\"\n","\n","    url = f\"https://api.themoviedb.org/3/movie/{movie_id}/keywords\"\n","\n","    headers = {\n","        \"accept\": \"application/json\",\n","        \"Authorization\": f\"Bearer {tmdb_key}\",\n","    }\n","\n","    response = requests.get(url, headers=headers)\n","\n","    if response.status_code == 200:\n","        return response.text\n","    return None"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T15:29:10.555149Z","iopub.status.busy":"2024-04-05T15:29:10.554626Z","iopub.status.idle":"2024-04-05T15:29:10.564861Z","shell.execute_reply":"2024-04-05T15:29:10.563838Z","shell.execute_reply.started":"2024-04-05T15:29:10.55512Z"},"trusted":true},"outputs":[],"source":["def parse_keywords(keywords: str) -> str:\n","    \"\"\"\n","    Parses the keywords into a list\n","    \"\"\"\n","\n","    keywords_dict = json.loads(keywords)\n","\n","    keywords_list = []\n","\n","    for item in keywords_dict[\"keywords\"]:\n","        keywords_list.append(item[\"name\"])\n","\n","    return \", \".join(keywords_list)"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T15:29:10.566578Z","iopub.status.busy":"2024-04-05T15:29:10.566236Z","iopub.status.idle":"2024-04-05T15:29:10.573927Z","shell.execute_reply":"2024-04-05T15:29:10.573039Z","shell.execute_reply.started":"2024-04-05T15:29:10.566549Z"},"trusted":true},"outputs":[],"source":["def handler_get_keywords(movie_id: int) -> list[int, str]:\n","    \"\"\"\n","    handles getting keywords\n","    \"\"\"\n","\n","    data = get_keywords(movie_id)\n","\n","    if not data:\n","        return None\n","\n","    keywords_str = parse_keywords(data)\n","\n","    return keywords_str"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T15:29:10.57839Z","iopub.status.busy":"2024-04-05T15:29:10.578072Z","iopub.status.idle":"2024-04-05T15:29:10.5898Z","shell.execute_reply":"2024-04-05T15:29:10.588714Z","shell.execute_reply.started":"2024-04-05T15:29:10.578362Z"},"trusted":true},"outputs":[],"source":["def process_movie_ids(movie_id: int, logger: logging, pbar: tqdm) -> None:\n","    global movies_collected\n","    \n","    if full_log:\n","        logger.info(f'Processing {movie_id}')\n","    \n","    # output file path for the current movie\n","    output_file = os.path.join(\n","        output_folder, f'scrapeTMDB_movies_{movie_id}.ndjson')\n","    with lock:\n","        pbar.update(1)\n","\n","    try:\n","        time.sleep(0.2)\n","        url = f\"https://api.themoviedb.org/3/movie/{movie_id}?language=en-US\"\n","        headers = {\n","            \"accept\": \"application/json\",\n","            \"Authorization\": f\"Bearer {tmdb_key}\"\n","        }\n","        response = requests.get(url, headers=headers)\n","        \n","        if response.status_code >= 500 or response.status_code == 429:\n","            logger.warning(\n","                f\"movie {movie_id} - Got {response.status_code} response, retrying...\")\n","            time.sleep(1)\n","\n","        # Process the response if it is successful\n","        if response.status_code == 200:\n","            rqst_json = response.json()\n","            \n","            keywords = handler_get_keywords(movie_id)\n","            rqst_json[\"keywords\"] = keywords\n","            with open(output_file, 'a', encoding=\"utf-8\") as append_file:\n","                append_file.write(json.dumps(rqst_json) + '\\n')\n","\n","            # Increment the count of collected movies and save the movie ID as completed\n","            # this way, even if the code fails, we can restart where we left off\n","            # Not needed for daily updates, but if running from 0, this will be userful\n","            with lock:\n","                movies_collected += 1\n","                with open(saved_movie_tracker_path, 'a', encoding='utf-8') as completed_movie_id_file:\n","                    completed_movie_id_file.write(f'{movie_id}\\n')\n","\n","            if full_log:\n","                logger.info(f'movies collected: {movies_collected}')\n","\n","    except Exception as error:\n","        logger.error(\n","            f'Expection for {movie_id} in process_movie_ids: {error}')\n","            \n","    if full_log:\n","        logger.info(f'Completed movie {movie_id}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## function to combine all the threads into 1 ndjson file\n"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T15:29:10.591656Z","iopub.status.busy":"2024-04-05T15:29:10.590934Z","iopub.status.idle":"2024-04-05T15:29:10.604319Z","shell.execute_reply":"2024-04-05T15:29:10.60359Z","shell.execute_reply.started":"2024-04-05T15:29:10.591625Z"},"trusted":true},"outputs":[],"source":["def combine_threads(logger: logging) -> None:\n","    logger.info('Starting to combine threads')\n","    thread_dir = output_folder\n","\n","    count = 0\n","    file_names = os.listdir(thread_dir)\n","\n","    # open the save file\n","    with open(json_save_file_path, 'a', encoding='utf-8') as append_file:\n","        for file in file_names:\n","            read_file_path = os.path.join(thread_dir, file)\n","            with open(read_file_path, 'r', encoding='utf-8') as read_file:\n","                \n","                for line in read_file:\n","                    line_data = json.loads(line)\n","                    count += 1\n","                    # save the line to the main json\n","                    append_file.write(f'{json.dumps(line_data)}\\n')\n","\n","    logger.info(f'Finished combining threads. Total movies added: {count}')"]},{"cell_type":"markdown","metadata":{},"source":["## load the ndjson file and make a python list from the data\n"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T15:29:10.606376Z","iopub.status.busy":"2024-04-05T15:29:10.605973Z","iopub.status.idle":"2024-04-05T15:29:10.633211Z","shell.execute_reply":"2024-04-05T15:29:10.632218Z","shell.execute_reply.started":"2024-04-05T15:29:10.606339Z"},"trusted":true},"outputs":[],"source":["def load_json_data() -> list[dict]:\n","    \"\"\"This function will take the combined thread ndjson file, parse it, format it, and return all the movies as python list of dicts\"\"\"\n","    \n","    data = []\n","    unique_ids = set()\n","\n","    # load the data\n","    with open(json_save_file_path, 'r', encoding='utf-8') as input_file:\n","        variations_to_ignore = [\n","            None, \"\", \"NA\", \"N/A\", \"None\", \"na\", \"n/a\", \"NULL\", \"Not Available\"]\n","\n","        for line in input_file:\n","            line_in_json = json.loads(line)\n","\n","            def format_list_to_str(line_in_json: dict, formatted_data: dict) -> dict:\n","                # convert genres list to str\n","                \n","                genres_str = None\n","                genres_list = []\n","                try:\n","                    for genre in line_in_json['genres']:\n","                        if genre['name'] is not any(variations_to_ignore):\n","                            genres_list.append(genre['name'])\n","                    if len(genres_list) > 0:\n","                        genres_str = ', '.join(genres_list)\n","                except:\n","                    pass\n","\n","                formatted_data['genres'] = genres_str\n","\n","                # convert production_companies list to str\n","                production_companies_str = None\n","                production_companies_list = []\n","                try:\n","                    for company in line_in_json['production_companies']:\n","                        if company['name'] is not any(variations_to_ignore):\n","                            production_companies_list.append(company['name'])\n","                    if len(production_companies_list) > 0:\n","                        production_companies_str = ', '.join(\n","                            production_companies_list)\n","                except:\n","                    pass\n","\n","                formatted_data['production_companies'] = production_companies_str\n","\n","                # convert production_countries list to str\n","                production_countries_str = None\n","                production_countries_list = []\n","                try:\n","                    for country in line_in_json['production_countries']:\n","                        if country['name'] is not any(variations_to_ignore):\n","                            production_countries_list.append(country['name'])\n","                    if len(production_countries_list) > 0:\n","                        production_countries_str = ', '.join(\n","                            production_countries_list)\n","                except:\n","                    pass\n","\n","                formatted_data['production_countries'] = production_countries_str\n","\n","                # convert spoken_languages list to str\n","                spoken_languages_str = None\n","                spoken_languages_list = []\n","                try:\n","                    for language in line_in_json['spoken_languages']:\n","                        if language['english_name'] is not any(variations_to_ignore):\n","                            spoken_languages_list.append(\n","                                language['english_name'])\n","                    if len(spoken_languages_list) > 0:\n","                        spoken_languages_str = ', '.join(spoken_languages_list)\n","                except:\n","                    pass\n","\n","                formatted_data['spoken_languages'] = spoken_languages_str\n","\n","                return formatted_data\n","\n","            # format the data\n","\n","            formatted_data = {}\n","\n","            formatted_data[\"id\"] = int(\n","                line_in_json['id']) if line_in_json['id'] is not None else 0\n","\n","            formatted_data[\"title\"] = str(\n","                line_in_json['title']) if line_in_json['title'] is not any(variations_to_ignore) else None\n","            formatted_data[\"vote_average\"] = float(\n","                line_in_json['vote_average']) if line_in_json['vote_average'] is not None else 0.0\n","\n","            formatted_data[\"vote_count\"] = int(\n","                line_in_json['vote_count']) if line_in_json['vote_count'] is not None else 0\n","\n","            formatted_data[\"status\"] = str(\n","                line_in_json['status']) if line_in_json['status'] is not any(variations_to_ignore) else None\n","\n","            formatted_data[\"release_date\"] = str(\n","                line_in_json['release_date']) if line_in_json['release_date'] is not None and line_in_json['release_date'] != \"\" else '1500-01-01'\n","\n","            formatted_data[\"revenue\"] = int(\n","                line_in_json['revenue']) if line_in_json['revenue'] is not None else 0\n","\n","            formatted_data[\"runtime\"] = int(\n","                line_in_json['runtime']) if line_in_json['runtime'] is not None else 0\n","\n","            formatted_data[\"adult\"] = bool(\n","                line_in_json['adult']) if line_in_json['adult'] is not None else False\n","\n","            formatted_data[\"backdrop_path\"] = str(\n","                line_in_json['backdrop_path']) if line_in_json['backdrop_path'] is not any(variations_to_ignore) else None\n","\n","            formatted_data[\"budget\"] = int(\n","                line_in_json['budget']) if line_in_json['budget'] is not None else 0\n","\n","            formatted_data[\"homepage\"] = str(\n","                line_in_json['homepage']) if line_in_json['homepage'] is not any(variations_to_ignore) else None\n","\n","            formatted_data[\"imdb_id\"] = str(\n","                line_in_json['imdb_id']) if line_in_json['imdb_id'] is not any(variations_to_ignore) else None\n","\n","            formatted_data[\"original_language\"] = str(\n","                line_in_json['original_language']) if line_in_json['original_language'] is not any(variations_to_ignore) else None\n","\n","            formatted_data[\"original_title\"] = str(\n","                line_in_json['original_title']) if line_in_json['original_title'] is not any(variations_to_ignore) else None\n","\n","            formatted_data[\"overview\"] = str(\n","                line_in_json['overview']) if line_in_json['overview'] is not any(variations_to_ignore) else None\n","\n","            formatted_data[\"popularity\"] = float(\n","                line_in_json['popularity']) if line_in_json['popularity'] is not None else 0.0\n","\n","            formatted_data[\"poster_path\"] = str(\n","                line_in_json['poster_path']) if line_in_json['poster_path'] is not any(variations_to_ignore) else None\n","\n","            formatted_data[\"tagline\"] = str(\n","                line_in_json['tagline']) if line_in_json['tagline'] is not any(variations_to_ignore) else None\n","            \n","            formatted_data[\"keywords\"] = str(line_in_json['keywords'])\n","            try:\n","                final_formatted_data = format_list_to_str(\n","                    line_in_json, formatted_data)\n","            except Exception as error:\n","                continue\n","\n","            # remove and newline chracters\n","            updated_data = {}\n","            for key, value in final_formatted_data.items():\n","                if isinstance(value, str):  # Check if the value is a string\n","                    updated_value = value.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n","                    updated_data[key] = updated_value\n","                else:\n","                    updated_data[key] = value\n","\n","            # add the data to our local list\n","            if updated_data['id'] not in unique_ids:\n","                unique_ids.add(updated_data['id'])\n","                data.append(updated_data)\n","    \n","    return data"]},{"cell_type":"markdown","metadata":{},"source":["## Merge the current data with the old dataset\n"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T15:29:10.635776Z","iopub.status.busy":"2024-04-05T15:29:10.63537Z","iopub.status.idle":"2024-04-05T15:29:10.6471Z","shell.execute_reply":"2024-04-05T15:29:10.646137Z","shell.execute_reply.started":"2024-04-05T15:29:10.635705Z"},"trusted":true},"outputs":[],"source":["def merger(logger: logging) -> None:\n","    global dataset_df\n","    logger.info('starting to merge new data with old dataframe')\n","    \n","    # Load and format the json data\n","    data = load_json_data()\n","    \n","    # Create a DataFrame from the loaded JSON data\n","    df = pd.DataFrame(data)\n","    df.replace({\"Not Available\": pd.NA,\n","               \"1500-01-01\": pd.NA,\n","               None:pd.NA}, inplace=True)\n","    \n","    # Concatenate the current dataset_df with the new df to merge the data\n","    merged_df = pd.concat([dataset_df, df])\n","    sorted_df = merged_df.sort_values('vote_count', ascending=False)\n","\n","    # save the kaggle dataset\n","    sorted_df.to_csv(kaggle_file,\n","                     index=False, encoding='utf-8', quoting=csv.QUOTE_ALL)"]},{"cell_type":"markdown","metadata":{},"source":["## Create the kaggle metadata file\n"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T15:29:10.649109Z","iopub.status.busy":"2024-04-05T15:29:10.648745Z","iopub.status.idle":"2024-04-05T15:29:10.661771Z","shell.execute_reply":"2024-04-05T15:29:10.660665Z","shell.execute_reply.started":"2024-04-05T15:29:10.649072Z"},"trusted":true},"outputs":[],"source":["def create_kaggle_info_file(logger: logging) -> None:\n","    logger.info('starting to create kaggle info file')\n","    \n","    # Define the data to be included in the kaggle info file\n","    data = {\n","        \"id\": \"asaniczka/tmdb-movies-dataset-2023-930k-movies\"\n","    }\n","    \n","    # the location of the metadata file\n","    metadata_file_location = os.path.join(\n","        kaggle_folder, 'dataset-metadata.json')\n","\n","    # Open the metadata file in write mode and write the data as JSON\n","    with open(metadata_file_location, 'w', encoding='utf-8') as metadat_file:\n","        json.dump(data, metadat_file)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Upload the dataset to kaggle\n"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T15:29:10.664113Z","iopub.status.busy":"2024-04-05T15:29:10.663007Z","iopub.status.idle":"2024-04-05T15:29:10.671902Z","shell.execute_reply":"2024-04-05T15:29:10.670682Z","shell.execute_reply.started":"2024-04-05T15:29:10.664071Z"},"trusted":true},"outputs":[],"source":["def upload_to_kaggle(logger) -> None:\n","    logger.info('starting to upload to kaggle')\n","    retries = 0\n","    while True and retries<5:\n","        try:\n","            command = f\"kaggle datasets version -p '{kaggle_folder}' -m '{date_today} Update' -r zip\"\n","            subprocess.run(command, shell=True, check=True)\n","            logger.info(\"Upload complete\")\n","            break\n","        except Exception as error:\n","            logger.error(f\"Error from Kaggle:{error}\")\n","            time.sleep(5)\n","            retries+=1"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T15:29:10.674212Z","iopub.status.busy":"2024-04-05T15:29:10.673276Z","iopub.status.idle":"2024-04-05T15:29:10.683982Z","shell.execute_reply":"2024-04-05T15:29:10.683118Z","shell.execute_reply.started":"2024-04-05T15:29:10.67418Z"},"trusted":true},"outputs":[],"source":["def executor() -> None:\n","    logger = setup_logger()\n","    logger.info('Starting Executor')\n","\n","    # first, lets get the id of the lastest movies\n","    latest = get_latest()\n","\n","    # second, let's get the last movie from our last run\n","    oldest = get_oldest()\n","    \n","    # Generate a list of movie IDs\n","    movie_ids_list = list(range(oldest, latest))\n","    total_movies_to_process = len(movie_ids_list)\n","    \n","    logger.info(\n","        f\"Total Movies to Process in this run: {total_movies_to_process}\")\n","    delete_old_files(logger)\n","    \n","    with tqdm(total=total_movies_to_process, unit=' movies') as pbar:\n","        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n","            futures = []\n","            for movie in movie_ids_list:\n","                futures.append(executor.submit(process_movie_ids, movie, logger, pbar))\n","            \n","            for future in concurrent.futures.as_completed(futures):\n","                try:\n","                    _ = future.result()\n","                except:\n","                    pass\n","    # combine all the threads to one file\n","    try:\n","        combine_threads(logger)\n","    except Exception as error:\n","        logger.critical(f\"Error when combining threads: {error}\")\n","        return\n","    \n","    # merge today's data with the old dataset\n","    try:\n","        merger(logger)\n","    except Exception as error:\n","        logger.critical(f\"Error when merging dataframes {error}\")\n","        return\n","    \n","    # create the kaggle metadata file\n","    try:\n","        create_kaggle_info_file(logger)\n","    except Exception as error:\n","        logger.critical(f\"Error when creating kaggle info file {error}\")\n","        return\n","    \n","    # upload the metadata file\n","    try:\n","        upload_to_kaggle(logger)\n","    except Exception as error:\n","        logger.critical(f\"Error when creating kaggle info file {error}\")\n","        return\n","\n","    logger.info('Completed Executor')"]},{"cell_type":"markdown","metadata":{},"source":["# Let's run this\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T15:29:10.685838Z","iopub.status.busy":"2024-04-05T15:29:10.685266Z","iopub.status.idle":"2024-04-05T15:29:58.720865Z","shell.execute_reply":"2024-04-05T15:29:58.719669Z","shell.execute_reply.started":"2024-04-05T15:29:10.685808Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'executor' is not defined","output_type":"error","traceback":["\u001b[31m---------------------------------------------------------------------------\u001b[39m","\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mexecutor\u001b[49m()\n","\u001b[31mNameError\u001b[39m: name 'executor' is not defined"]}],"source":["executor()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":8151780,"datasetId":3816617,"sourceId":8038449,"sourceType":"datasetVersion"}],"dockerImageVersionId":30558,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":4}
